task_num:6
con_layer1:128
con_layer1_filter:8
con_layer2:64
con_layer2_filter:4
lstm_layer:64
drop:0.2
r_drop:0.2
l2_value:0.001
shared_layer:576
dense_num:64
Accuracy: 0.9409060175794456 TPR: 0.8961832061068702 TNR: 0.9429784223558543 Bacc: 0.9195808142313622
Accuracy: 0.9241379310344827 TPR: 0.7128440366972477 TNR: 0.940948905109489 Bacc: 0.8268964709033684
Accuracy: 0.8762339418526031 TPR: 0.7545033453422543 TNR: 0.8847921264970873 Bacc: 0.8196477359196708
Accuracy: 0.9335023664638269 TPR: 0.981058495821727 TNR: 0.930430088177074 Bacc: 0.9557442919994005
Accuracy: 0.9598377281947261 TPR: 0.7513134851138353 TNR: 0.9726558294757239 Bacc: 0.8619846572947796
Accuracy: 0.9191007437457742 TPR: 0.6544837980406933 TNR: 0.945183094406893 Bacc: 0.7998334462237932
sum_bacc: 5.183687416572375
sum_TPR: 4.750386367122627



epoch = 80

def attention_3d_block(shared,inputs):
    # inputs.shape = (batch_size, time_steps, input_dim)
    input_dim = int(inputs.shape[2])
    activation_weights= Flatten()(shared)
    activation_weights=Dense(TIME_STEPS,activation='tanh')(activation_weights)
    activation_weights=Activation('softmax')(activation_weights)
    activation_weights= RepeatVector(input_dim)(activation_weights)
    activation_weights=Permute([2,1])(activation_weights)
    activation_weighted=merge([inputs,activation_weights], mode = 'mul')

    # sum_weighted = Lambda(lambda x: K.sum(x, axis=-2), output_shape=(input_dim,))(activation_weighted)
    return activation_weighted

def build_model(data1,data2,data3,data4,data5,data6,
                task_num, con_layer1, con_layer1_filter, con_layer2, con_layer2_filter,
                lstm_layer, drop, r_drop, l2_value, shared_layer,dense_num, n_labels):
    """
    Keras Function model
    """

    concate_list = []
    input_list = []
    for i in range(0,task_num):
        locals()['input'+str(i)] = Input(shape=(locals()['data'+ str(i+1)].shape[1], locals()['data'+ str(i+1)].shape[2]), name='input'+str(i))
        locals()['cnn_out'+str(i)] = Convolution1D(nb_filter=con_layer1, filter_length=con_layer1_filter, activation='relu')(locals()['input'+str(i)])
        # locals()['cnn_out'+str(i)] = Convolution1D(nb_filter=con_layer2, filter_length=con_layer2_filter, activation='relu')(locals()['cnn_out'+str(i)])
        # locals()['lstm_out'+str(i)] = LSTM(lstm_layer, activation='relu', dropout=drop,
        #                                    recurrent_dropout=r_drop,kernel_regularizer=regularizers.l2(l2_value),return_sequences=True)(locals()['input'+str(i)])
        concate_list.append(locals()['cnn_out'+str(i)])
        input_list.append(locals()['input'+str(i)])

    concate_layer = keras.layers.concatenate(concate_list)

    # lstm_shared = LSTM(shared_layer, activation='relu',dropout=drop, recurrent_dropout=r_drop,
    #                    return_sequences=True)(concate_layer)


    output_list = []
    for i in range(0,task_num):
        locals()['concate_layer'+str(i)] = attention_3d_block(concate_layer,locals()['cnn_out'+str(i)])
        locals()['flatten_layer'+str(i)] = LSTM(dense_num,activation='relu')(locals()['concate_layer'+str(i)])
        locals()['sub'+str(i)] = Dense(dense_num,activation='relu')(locals()['flatten_layer'+str(i)])
        locals()['out'+str(i)] = Dense(n_labels, activation='sigmoid')(locals()['sub'+str(i)])
        output_list.append(locals()['out'+str(i)])

    model = Model(inputs=input_list,outputs=output_list)
    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['binary_accuracy'])
